{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lookup from /om/user/ehoseini/miniconda3/lib/python3.7/site-packages/brainio_collection/lookup.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/om/user/ehoseini/miniconda3/lib/python3.7/site-packages/brainscore/metrics/__init__.py:37: FutureWarning: xarray subclass Score should explicitly define __slots__\n",
      "  class Score(DataAssembly):\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "from utils import extract_pool,model_grps_config\n",
    "import utils.optim_utils\n",
    "importlib.reload(utils.optim_utils)\n",
    "from utils.optim_utils import optim, optim_pool, pt_create_corr_rdm_short\n",
    "from utils.data_utils import load_obj, SAVE_DIR, UD_PARENT, RESULTS_DIR, LEX_PATH_SET, save_obj, ANALYZE_DIR\n",
    "import torch \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.cm as cm \n",
    "from tqdm import tqdm\n",
    "import tsnecuda\n",
    "import seaborn as sns \n",
    "import pandas as pd \n",
    "import os \n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from tqdm import tqdm\n",
    "import tqdm.notebook as tq\n",
    "from tqdm import tqdm_notebook\n",
    "import fnmatch \n",
    "import re\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_id=['group=gpt2-xl_layer_compare_v1-dataset=ud_sentencez_token_filter_v3-activation-bench=None-ave=False',\n",
    "#            'group=ctrl_layer_compare_v1-dataset=ud_sentencez_token_filter_v3-activation-bench=None-ave=False',\n",
    "#            'group=bert-large-uncased-whole-word-masking_layer_compare_v1-dataset=ud_sentencez_token_filter_v3-activation-bench=None-ave=False',\n",
    "#            'group=albert-xxlarge-v2_layer_compare_v1-dataset=ud_sentencez_token_filter_v3-activation-bench=None-ave=False',\n",
    "#            'group=roberta-base_layer_compare_v1-dataset=ud_sentencez_token_filter_v3-activation-bench=None-ave=False',\n",
    "#            'group=xlm-mlm-en-2048_layer_compare_v1-dataset=ud_sentencez_token_filter_v3-activation-bench=None-ave=False',\n",
    "#            'group=xlnet-large-cased_layer_compare_v1-dataset=ud_sentencez_token_filter_v3-activation-bench=None-ave=False']\n",
    "\n",
    "extract_id=['group=gpt2-xl_layer_compare_v1-dataset=coca_spok_filter_punct_10K_sample_2-activation-bench=None-ave=False']\n",
    "\n",
    "\n",
    "optim_id=['coordinate_ascent_eh-obj=D_s-n_iter=1000-n_samples=50-n_init=2-run_gpu=True',\n",
    "          'coordinate_ascent_eh-obj=D_s_var-n_iter=1000-n_samples=50-n_init=2-run_gpu=True']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_files=[]\n",
    "optim_results=[]\n",
    "for ext in extract_id:\n",
    "    for optim in optim_id:\n",
    "        optim_file=os.path.join(RESULTS_DIR,f\"results_{ext}_{optim}.pkl\")\n",
    "        optim_files.append(optim_file)\n",
    "        optim_results.append(load_obj(optim_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[15,15])\n",
    "ax = fig.add_axes([.1,.1,.4,.6])\n",
    "\n",
    "cmap=cm.get_cmap('viridis_r')\n",
    "\n",
    "alph_col=cmap(np.divide(range(len(optim_results)),len(optim_results)))\n",
    "tick_l=[]\n",
    "tick=[]\n",
    "for idx, res in enumerate(optim_results):\n",
    "    ax.barh(idx,res['optimized_d'],color=alph_col[[idx],:],label=res['optimizatin_name'])\n",
    "    ext_obj=extract_pool[res['extractor_name']]()\n",
    "    str_val=\"{:.5f}\".format(res['optimized_d'])\n",
    "    print(f\"{str_val}\")\n",
    "    optim_type=re.search('obj=\\w+-',res['optimizatin_name'])[0][0:-1]\n",
    "    tick_l.append(f\" {np.unique(ext_obj.model_spec)[0]}, {optim_type}, s: {len(res['optimized_S'])} \\n {ext_obj.dataset}  ,  value:{str_val}\")\n",
    "    tick.append(idx)\n",
    "\n",
    "\n",
    "#ax.set_xlabel(f\"D_s \\n\\n  models:{ext_obj.model_spec}\\nlayers:{ext_obj.layer_spec} averaging : {ext_obj.average_sentence}\",fontsize=12)\n",
    "ax.set_yticklabels(tick_l,fontsize=12)\n",
    "ax.set_yticks(tick)\n",
    "ax.set_title(res['optimizatin_name'],fontsize=12)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "#ax.legend(bbox_to _anchor=(1.1, .85), frameon=True,fontsize=12)\n",
    "ax.invert_yaxis()\n",
    "#fig.savefig(os.path.join(Analysis_path,'DV_test_gamma_alpha_is_0.pdf'))\n",
    "plt.savefig(os.path.join(ANALYZE_DIR,f\"U01_SET2_optimization_results.png\"), dpi=None, facecolor='w', edgecolor='w',\n",
    "       orientation='portrait',transparent=True, bbox_inches=None, pad_inches=0.1,frameon=False)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = plt.figure(figsize=[15,15])\n",
    "ax = fig.add_axes([.1,.1,.4,.6])\n",
    "\n",
    "cmap=cm.get_cmap('viridis_r')\n",
    "\n",
    "alph_col=cmap(np.divide(range(len(optim_results)),len(optim_results)))\n",
    "tick_l=[]\n",
    "tick=[]\n",
    "for idx, res in enumerate(optim_results):\n",
    "    \n",
    "    #ax.barh(idx,res['optimized_d'],color=alph_col[[idx],:],label=res['optimizatin_name'])\n",
    "    ext_obj=extract_pool[res['extractor_name']]()\n",
    "    ext_obj.load_dataset()\n",
    "    optimizer_obj = optim_pool[res['optimizatin_name']]()\n",
    "    optimizer_obj.load_extractor(ext_obj)\n",
    "    optimizer_obj.precompute_corr_rdm_on_gpu()\n",
    "    str_val=\"{:.5f}\".format(res['optimized_d'])\n",
    "    print(f\"{str_val}\")\n",
    "    optim_type=re.search('obj=\\w+-',res['optimizatin_name'])[0][0:-1]\n",
    "    tick_l.append(f\" {np.unique(ext_obj.model_spec)[0]}, {optim_type}, s: {len(res['optimized_S'])} \\n {ext_obj.dataset}  ,  value:{str_val}\")\n",
    "    tick.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_obj.load_dataset()\n",
    "ext_obj()\n",
    "optimizer_obj = optim_pool[res['optimizatin_name']]()\n",
    "optimizer_obj.load_extractor(ext_obj)\n",
    "\n",
    "optimizer_obj.precompute_corr_rdm_on_gpu()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter=100\n",
    "num_samples=50\n",
    "\n",
    "for idx, res in enumerate(optim_results):\n",
    "    if idx>0:\n",
    "        ext_obj=extract_pool[res['extractor_name']]()\n",
    "        mdl_name=np.unique(ext_obj.model_spec)[0]\n",
    "        group=f'{mdl_name}_layers'\n",
    "        extractor_id=f'group={group}-dataset={ext_obj.dataset}-{ext_obj.extract_type}-bench=None-ave={ext_obj.average_sentence}'\n",
    "        extractor_obj = extract_pool[extractor_id]()\n",
    "        extractor_obj.load_dataset()\n",
    "        model_layers = extractor_obj.layer_name\n",
    "        extractor_obj()\n",
    "        mdl_name=str(np.unique(extractor_obj.model_spec).squeeze())\n",
    "        optim_obj=optim_pool[res['optimizatin_name']]()\n",
    "        optim_obj.load_extractor(extractor_obj)\n",
    "        layer_id_list=[x['layer'] for x in optim_obj.activations]\n",
    "        del extractor_obj\n",
    "        activation_list=[]\n",
    "        var_explained=[]\n",
    "        loadings=[]\n",
    "        components=[]\n",
    "        pca_type='fixed'\n",
    "        for idx, act_dict in tqdm(enumerate(optim_obj.activations)):\n",
    "            act=torch.tensor(act_dict['activations'], dtype=float, device=optim_obj.device, requires_grad=False)\n",
    "    # act must be in m sample * n feature shape ,\n",
    "            u,s,v=torch.pca_lowrank(act,q=200)\n",
    "    # keep 85% variance explained ,\n",
    "            idx_85=torch.cumsum(s**2,dim=0)/torch.sum(s**2)<.85\n",
    "            cols=list(torch.where(idx_85)[0].cpu().numpy())\n",
    "            if pca_type=='fixed':\n",
    "                act_pca = torch.matmul(act, v[:, :100])\n",
    "            elif pca_type=='equal_var':\n",
    "                act_pca = torch.matmul(act, v[:, cols])\n",
    "        \n",
    "            activation_list.append(act_pca)\n",
    "            var_explained.append(torch.cumsum(torch.cat((torch.tensor([0],device=optim_obj.device),s**2)),dim=0)/torch.sum(s**2))\n",
    "    #var_explained.append(torch.cumsum(s**2,dim=0)/torch.sum(s**2))\n",
    "        var_explained=torch.stack(var_explained).cpu()\n",
    "  \n",
    "        total_sent=activation_list[0].shape[0]\n",
    "        act_list_norm=[(X-X.mean(axis=1,keepdim=True)) for X in activation_list]\n",
    "        act_list_norm=[torch.nn.functional.normalize(X) for X in act_list_norm]\n",
    "        layer_dist=[]\n",
    "        for idx in tqdm_notebook(range(len(activation_list))):\n",
    "            pair_dist=[]\n",
    "            for idy in tqdm_notebook(range(len(activation_list)),position=1):\n",
    "                sample_dist=[]\n",
    "                pair_list_norm=[act_list_norm[idx],act_list_norm[idy]]\n",
    "                XY_corr_list = [torch.tensor(1, device=X.device, dtype=float) - torch.mm(X, torch.transpose(X, 1, 0)) for X in\n",
    "                            pair_list_norm]\n",
    "                for sample_iter in range(num_iter):\n",
    "                    samples=torch.tensor(np.random.choice(total_sent,num_samples,replace=False), dtype = torch.long, device = act_list_norm[0].device)\n",
    "                    pairs = torch.combinations(samples, with_replacement=False)\n",
    "                    XY_corr_sample = [XY_corr[pairs[:, 0], pairs[:, 1]] for XY_corr in XY_corr_list]\n",
    "                    XY_corr_sample_tensor = torch.stack(XY_corr_sample)\n",
    "                    XY_corr_sample_tensor = torch.transpose(XY_corr_sample_tensor, 1, 0)\n",
    "                    if XY_corr_sample_tensor.shape[1] < XY_corr_sample_tensor.shape[0]:\n",
    "                        XY_corr_sample_tensor = torch.transpose(XY_corr_sample_tensor, 1, 0)\n",
    "                    assert (XY_corr_sample_tensor.shape[1] > XY_corr_sample_tensor.shape[0])\n",
    "                    d_mat = pt_create_corr_rdm_short(XY_corr_sample_tensor, device=samples.device)\n",
    "                #n1 = d_mat.shape[1],\n",
    "                #correction = n1 * n1 / (n1 * (n1 - 1) / 2),\n",
    "                #d_val = correction * d_mat.mean(dim=(0, 1)),\n",
    "                    d_val = d_mat[0,1]\n",
    "                    sample_dist.append(d_val)\n",
    "                pair_dist.append(torch.stack(sample_dist))\n",
    "            layer_dist.append(pair_dist)\n",
    "            \n",
    "        optim_act_list_norm=[x[res['optimized_S'],:] for x in act_list_norm]\n",
    "        layer_similarity=[pt_create_corr_rdm_short(x) for x in optim_act_list_norm]\n",
    "        optim_pairs = torch.combinations(torch.tensor(np.arange(len(res['optimized_S']))), with_replacement=False)\n",
    "        layer_optim_dist=[]\n",
    "        for idx in tqdm_notebook(range(len(activation_list))):\n",
    "            pair_optim_dist=[]\n",
    "            for idy in tqdm_notebook(range(len(activation_list)),position=1):\n",
    "                pair_similarity=[layer_similarity[idx],layer_similarity[idy]]\n",
    "                XY_corr_sample = [XY_corr[optim_pairs[:, 0], optim_pairs[:, 1]] for XY_corr in pair_similarity]\n",
    "                XY_corr_sample_tensor=torch.stack(XY_corr_sample)\n",
    "                d_mat = pt_create_corr_rdm_short(XY_corr_sample_tensor, device=XY_corr_sample_tensor.device)\n",
    "                d_val =d_mat[0,1].cpu()\n",
    "                pair_optim_dist.append([d_val])\n",
    "            layer_optim_dist.append(pair_optim_dist)\n",
    "        print(\"Done!\")\n",
    "        pereira_settings=extract_pool['group=best_performing_pereira_1-dataset=ud_sentences-activation-bench=None-ave=False']()\n",
    "\n",
    "        try :\n",
    "            model_loc = pereira_settings.model_spec.index(mdl_name)\n",
    "            pereira_layer_id=pereira_settings.layer_spec[model_loc]\n",
    "        except ValueError as e:\n",
    "            pereira_layer_id=np.argmax(score_score)\n",
    "        \n",
    "        \n",
    "        Pereira_dist=torch.mean(torch.stack(layer_dist[pereira_layer_id]),dim=1)\n",
    "        dist_val,dist_idx=torch.sort(Pereira_dist)\n",
    "        assert(dist_idx[0]==pereira_layer_id)\n",
    "        cuts=np.linspace(dist_val.cpu().numpy().min(),dist_val.cpu().numpy().max(),4,endpoint=False)\n",
    "        Pereira_dist_optim=torch.tensor([torch.stack([x[0] for x in y]) for y in layer_optim_dist][pereira_layer_id])\n",
    "        dist_val_optim,dist_idx_optim=torch.sort(Pereira_dist_optim)\n",
    "        Pereira_ordered=Pereira_dist_optim[dist_idx]\n",
    "        assert(dist_idx[0]==pereira_layer_id)\n",
    "\n",
    "        fig = plt.figure(figsize=(8,8*1.5))\n",
    "\n",
    "        ax = fig.add_axes((.1,.4,.3*1.5,.3))\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes('right', size=.05, pad=0.1)\n",
    "        im = ax.imshow(torch.stack([torch.stack([x.mean() for x in y]) for y in layer_dist]).cpu(),aspect='auto',interpolation='none')\n",
    "        ax.set_yticks(np.arange(var_explained.shape[0]))\n",
    "        ax.set_xticks(np.arange(var_explained.shape[0]))\n",
    "        ax.set_yticklabels([f\" {model_layers[idx]}\" for idx,x in enumerate(var_explained) ])\n",
    "        ax.set_xticklabels([f\"{model_layers[idx]}\" for idx,x in enumerate(var_explained) ],rotation=90)\n",
    "\n",
    "        cbar = fig.colorbar(im, cax=cax)\n",
    "\n",
    "\n",
    "        ax = fig.add_axes((.65,.4,.3*1.5,.3))\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes('right', size=.05, pad=0.1)\n",
    "        im = ax.imshow(torch.stack([torch.stack([x[0] for x in y]) for y in layer_optim_dist]).cpu(),aspect='auto',interpolation='none')\n",
    "        ax.set_yticks(np.arange(var_explained.shape[0]))\n",
    "        ax.set_xticks(np.arange(var_explained.shape[0]))\n",
    "        ax.set_yticklabels([ ])\n",
    "        ax.set_xticklabels([ ],rotation=90)\n",
    "\n",
    "        cbar = fig.colorbar(im, cax=cax)\n",
    "        ax = fig.add_axes((.2,.1,.8,.12))\n",
    "        ax.scatter(np.arange(dist_val.cpu().shape[0]),dist_val.cpu(),zorder=3)\n",
    "        ax.scatter(np.arange(dist_val_optim.cpu().shape[0]),Pereira_ordered.cpu(),zorder=4)\n",
    "\n",
    "        ax.set_xlim((-1,dist_val_optim.cpu().shape[0]+1))\n",
    "        ax.set_ylim((0-.05,np.max(dist_val_optim.cpu().numpy())+.05))\n",
    "        #[ax.plot(plt.xlim(),[x,x],'k--') for x in cuts],\n",
    "        #closest_points=[np.argmin(np.abs(dist_val.cpu()-x)) for x in cuts],\n",
    "        #optimized_dist=torch.tensor([dist_val_optim[dist_idx_optim==x] for x in extractor_obj.layer_spec]),\n",
    "        #optimized_dist_loc=torch.tensor([torch.where(dist_idx_optim==x) for x in extractor_obj.layer_spec]),\n",
    "        #ax.scatter(optimized_dist_loc.cpu(),optimized_dist.cpu(),50,color=(0,0,0)),\n",
    "        closest_points=[int(np.where(dist_idx.cpu().numpy()==x)[0]) for x in res['layer_spec']]\n",
    "        [ax.scatter(x,dist_val[x].cpu().numpy(),50,color=(0,0,0),zorder=5) for x in closest_points]\n",
    "        [ax.scatter(x,Pereira_ordered[x].cpu().numpy(),50,color=(1,0,0),zorder=5) for x in closest_points]\n",
    "\n",
    "        ax.set_xticks(tuple(np.arange(dist_val.cpu().shape[0])))\n",
    "        ax.set_xticklabels(dist_idx.cpu().numpy())\n",
    "        ax.set_xticklabels([model_layers[int(x)] for x in dist_idx.cpu().numpy()],rotation=90)\n",
    "        [ax.plot([x,x],plt.ylim(),'k-',zorder=2) for x in closest_points]\n",
    "        \n",
    "        ax.set_title( f\"{res['extractor_name']} , \\n {res['optimizatin_name']}\",fontsize=12)\n",
    "        \n",
    "        plt.savefig(os.path.join(ANALYZE_DIR,f\"{res['extractor_name']}_{res['optimizatin_name']}_RDM.png\"), dpi=None, facecolor='w', edgecolor='w',\n",
    "       orientation='portrait',transparent=True, bbox_inches=None, pad_inches=0.1,frameon=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{res['extractor_name']}_{res['optimizatin_name']}_RDM.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "fig = plt.figure(figsize=(10,15))\n",
    "ax = fig.add_axes((.1,.4,.5*1.5,.5))\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size=.1, pad=0.5)\n",
    "im = ax.imshow(torch.stack([torch.stack([x.mean() for x in y]) for y in layer_dist]).cpu(),aspect='auto',interpolation='none')\n",
    "ax.set_yticks(np.arange(var_explained.shape[0]))\n",
    "ax.set_xticks(np.arange(var_explained.shape[0]))\n",
    "#ax.set_yticklabels([f\" {model_layers[idx]}\" for idx,x in enumerate(var_explained) ])\n",
    "#ax.set_xticklabels([f\"{model_layers[idx]}\" for idx,x in enumerate(var_explained) ],rotation=90)\n",
    "\n",
    "\n",
    "ax = fig.add_axes((.1,.1,.5*1.37,.18))\n",
    "ax.set_title(f\"{res['extractor_name']}_num_samples_{num_samples}_num_iter_{num_iter}\")\n",
    "ax.scatter(np.arange(dist_val.cpu().shape[0]),dist_val.cpu())\n",
    "ax.set_xlim((-1,dist_val.cpu().shape[0]+1))\n",
    "ax.set_ylim((0-.05,np.max(dist_val.cpu().numpy())+.05))\n",
    "[ax.plot(plt.xlim(),[x,x],'k--') for x in cuts],\n",
    "closest_points=[np.argmin(np.abs(dist_val.cpu()-x)) for x in cuts]\n",
    "[ax.scatter(x.cpu().numpy(),dist_val[int(x.cpu().numpy())].cpu().numpy(),60,color=(0,0,0)) for x in closest_points]\n",
    "ax.set_xticks(tuple(np.arange(dist_val.cpu().shape[0])))\n",
    "    #ax.set_xticklabels(dist_idx.cpu().numpy()),\n",
    "ax.set_xticklabels([model_layers[int(x)] for x in dist_idx.cpu().numpy()],rotation=90)\n",
    "[ax.plot([x.cpu().numpy(),x.cpu().numpy()],plt.ylim(),'k-') for x in closest_points]\n",
    "ax.set_xlim((0-.5,len(dist_idx)-.5))\n",
    "#plt.savefig(os.path.join(ANALYZE_DIR,f\"{extractor_id}_num_samples_{num_samples}_num_iter_{num_iter}_layerwise_similiarty_dist_vs_score.png\"), dpi=None, facecolor='w', edgecolor='w',\n",
    "#       orientation='portrait',transparent=True, bbox_inches=None, pad_inches=0.1,frameon=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_obj=extract_pool[res['extractor_name']]()\n",
    "mdl_name=np.unique(ext_obj.model_spec)[0]\n",
    "group=f'{mdl_name}_layers'\n",
    "extractor_id=f'group={group}-dataset={ext_obj.dataset}-{ext_obj.extract_type}-bench=None-ave={ext_obj.average_sentence}'\n",
    "extractor_obj = extract_pool[extractor_id]()\n",
    "extractor_obj.load_dataset()\n",
    "model_layers = extractor_obj.layer_name\n",
    "extractor_obj()\n",
    "mdl_name=str(np.unique(extractor_obj.model_spec).squeeze())\n",
    "optim_obj=optim_pool[res['optimizatin_name']]()\n",
    "optim_obj.load_extractor(extractor_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_id_list=[x['layer'] for x in optim_obj.activations]\n",
    "del extractor_obj\n",
    "activation_list=[]\n",
    "var_explained=[]\n",
    "loadings=[]\n",
    "components=[]\n",
    "pca_type='fixed'\n",
    "for idx, act_dict in tqdm(enumerate(optim_obj.activations)):\n",
    "    act=torch.tensor(act_dict['activations'], dtype=float, device=optim_obj.device, requires_grad=False)\n",
    "# act must be in m sample * n feature shape ,\n",
    "    u,s,v=torch.pca_lowrank(act,q=200)\n",
    "# keep 85% variance explained ,\n",
    "    idx_85=torch.cumsum(s**2,dim=0)/torch.sum(s**2)<.85\n",
    "    cols=list(torch.where(idx_85)[0].cpu().numpy())\n",
    "    if pca_type=='fixed':\n",
    "        act_pca = torch.matmul(act, v[:, :100])\n",
    "    elif pca_type=='equal_var':\n",
    "        act_pca = torch.matmul(act, v[:, cols])\n",
    "\n",
    "    activation_list.append(act_pca)\n",
    "    var_explained.append(torch.cumsum(torch.cat((torch.tensor([0],device=optim_obj.device),s**2)),dim=0)/torch.sum(s**2))\n",
    "#var_explained.append(torch.cumsum(s**2,dim=0)/torch.sum(s**2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_explained=torch.stack(var_explained).cpu()\n",
    "\n",
    "total_sent=activation_list[0].shape[0]\n",
    "act_list_norm=[(X-X.mean(axis=1,keepdim=True)) for X in activation_list]\n",
    "act_list_norm=[torch.nn.functional.normalize(X) for X in act_list_norm]\n",
    "layer_dist=[]\n",
    "for idx in tqdm_notebook(range(len(activation_list))):\n",
    "    pair_dist=[]\n",
    "    for idy in tqdm_notebook(range(len(activation_list)),position=1):\n",
    "        sample_dist=[]\n",
    "        pair_list_norm=[act_list_norm[idx],act_list_norm[idy]]\n",
    "        XY_corr_list = [torch.tensor(1, device=X.device, dtype=float) - torch.mm(X, torch.transpose(X, 1, 0)) for X in\n",
    "                    pair_list_norm]\n",
    "        for sample_iter in range(num_iter):\n",
    "            samples=torch.tensor(np.random.choice(total_sent,num_samples,replace=False), dtype = torch.long, device = act_list_norm[0].device)\n",
    "            pairs = torch.combinations(samples, with_replacement=False)\n",
    "            XY_corr_sample = [XY_corr[pairs[:, 0], pairs[:, 1]] for XY_corr in XY_corr_list]\n",
    "            XY_corr_sample_tensor = torch.stack(XY_corr_sample)\n",
    "            XY_corr_sample_tensor = torch.transpose(XY_corr_sample_tensor, 1, 0)\n",
    "            if XY_corr_sample_tensor.shape[1] < XY_corr_sample_tensor.shape[0]:\n",
    "                XY_corr_sample_tensor = torch.transpose(XY_corr_sample_tensor, 1, 0)\n",
    "            assert (XY_corr_sample_tensor.shape[1] > XY_corr_sample_tensor.shape[0])\n",
    "            d_mat = pt_create_corr_rdm_short(XY_corr_sample_tensor, device=samples.device)\n",
    "        #n1 = d_mat.shape[1],\n",
    "        #correction = n1 * n1 / (n1 * (n1 - 1) / 2),\n",
    "        #d_val = correction * d_mat.mean(dim=(0, 1)),\n",
    "            d_val = d_mat[0,1]\n",
    "            sample_dist.append(d_val)\n",
    "        pair_dist.append(torch.stack(sample_dist))\n",
    "    layer_dist.append(pair_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_act_list_norm=[x[res['optimized_S'],:] for x in act_list_norm]\n",
    "layer_similarity=[pt_create_corr_rdm_short(x) for x in optim_act_list_norm]\n",
    "optim_pairs = torch.combinations(torch.tensor(np.arange(len(res['optimized_S']))), with_replacement=False)\n",
    "layer_optim_dist=[]\n",
    "for idx in tqdm_notebook(range(len(activation_list))):\n",
    "    pair_optim_dist=[]\n",
    "    for idy in tqdm_notebook(range(len(activation_list)),position=1):\n",
    "        pair_similarity=[layer_similarity[idx],layer_similarity[idy]]\n",
    "        XY_corr_sample = [XY_corr[optim_pairs[:, 0], optim_pairs[:, 1]] for XY_corr in pair_similarity]\n",
    "        XY_corr_sample_tensor=torch.stack(XY_corr_sample)\n",
    "        d_mat = pt_create_corr_rdm_short(XY_corr_sample_tensor, device=XY_corr_sample_tensor.device)\n",
    "        d_val =d_mat[0,1].cpu()\n",
    "        pair_optim_dist.append([d_val])\n",
    "    layer_optim_dist.append(pair_optim_dist)\n",
    "print(\"Done!\")\n",
    "pereira_settings=extract_pool['group=best_performing_pereira_1-dataset=ud_sentences-activation-bench=None-ave=False']()\n",
    "\n",
    "try :\n",
    "    model_loc = pereira_settings.model_spec.index(mdl_name)\n",
    "    pereira_layer_id=pereira_settings.layer_spec[model_loc]\n",
    "except ValueError as e:\n",
    "    pereira_layer_id=np.argmax(score_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pereira_dist=torch.mean(torch.stack(layer_dist[pereira_layer_id]),dim=1)\n",
    "dist_val,dist_idx=torch.sort(Pereira_dist)\n",
    "assert(dist_idx[0]==pereira_layer_id)\n",
    "cuts=np.linspace(dist_val.cpu().numpy().min(),dist_val.cpu().numpy().max(),4,endpoint=False)\n",
    "Pereira_dist_optim=torch.tensor([torch.stack([x[0] for x in y]) for y in layer_optim_dist][pereira_layer_id])\n",
    "dist_val_optim,dist_idx_optim=torch.sort(Pereira_dist_optim)\n",
    "Pereira_ordered=Pereira_dist_optim[dist_idx]\n",
    "assert(dist_idx[0]==pereira_layer_id)\n",
    "\n",
    "fig = plt.figure(figsize=(8,8*1.5))\n",
    "\n",
    "ax = fig.add_axes((.1,.4,.3*1.5,.3))\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size=.05, pad=0.1)\n",
    "im = ax.imshow(torch.stack([torch.stack([x.mean() for x in y]) for y in layer_dist]).cpu(),aspect='auto',interpolation='none')\n",
    "ax.set_yticks(np.arange(var_explained.shape[0]))\n",
    "ax.set_xticks(np.arange(var_explained.shape[0]))\n",
    "ax.set_yticklabels([f\" {model_layers[idx]}\" for idx,x in enumerate(var_explained) ])\n",
    "ax.set_xticklabels([f\"{model_layers[idx]}\" for idx,x in enumerate(var_explained) ],rotation=90)\n",
    "\n",
    "cbar = fig.colorbar(im, cax=cax)\n",
    "\n",
    "\n",
    "ax = fig.add_axes((.65,.4,.3*1.5,.3))\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size=.05, pad=0.1)\n",
    "im = ax.imshow(torch.stack([torch.stack([x[0] for x in y]) for y in layer_optim_dist]).cpu(),aspect='auto',interpolation='none')\n",
    "ax.set_yticks(np.arange(var_explained.shape[0]))\n",
    "ax.set_xticks(np.arange(var_explained.shape[0]))\n",
    "ax.set_yticklabels([ ])\n",
    "ax.set_xticklabels([ ],rotation=90)\n",
    "\n",
    "cbar = fig.colorbar(im, cax=cax)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax = fig.add_axes((.2,.1,.8,.15))\n",
    "ax.scatter(np.arange(dist_val.cpu().shape[0]),dist_val.cpu(),zorder=3)\n",
    "ax.scatter(np.arange(dist_val_optim.cpu().shape[0]),Pereira_ordered.cpu(),zorder=4)\n",
    "\n",
    "ax.set_xlim((-1,dist_val_optim.cpu().shape[0]+1))\n",
    "ax.set_ylim((0-.05,np.max(dist_val_optim.cpu().numpy())+.05))\n",
    "#[ax.plot(plt.xlim(),[x,x],'k--') for x in cuts],\n",
    "#closest_points=[np.argmin(np.abs(dist_val.cpu()-x)) for x in cuts],\n",
    "#optimized_dist=torch.tensor([dist_val_optim[dist_idx_optim==x] for x in extractor_obj.layer_spec]),\n",
    "#optimized_dist_loc=torch.tensor([torch.where(dist_idx_optim==x) for x in extractor_obj.layer_spec]),\n",
    "#ax.scatter(optimized_dist_loc.cpu(),optimized_dist.cpu(),50,color=(0,0,0)),\n",
    "closest_points=[int(np.where(dist_idx.cpu().numpy()==x)[0]) for x in res['layer_spec']]\n",
    "[ax.scatter(x,dist_val[x].cpu().numpy(),50,color=(0,0,0),zorder=5) for x in closest_points]\n",
    "[ax.scatter(x,Pereira_ordered[x].cpu().numpy(),50,color=(1,0,0),zorder=5) for x in closest_points]\n",
    "\n",
    "ax.set_xticks(tuple(np.arange(dist_val.cpu().shape[0])))\n",
    "ax.set_xticklabels(dist_idx.cpu().numpy())\n",
    "ax.set_xticklabels([model_layers[int(x)] for x in dist_idx.cpu().numpy()],rotation=90)\n",
    "[ax.plot([x,x],plt.ylim(),'k-',zorder=2) for x in closest_points]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
