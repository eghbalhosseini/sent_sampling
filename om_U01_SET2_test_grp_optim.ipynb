{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lookup from /usr/local/lib/python3.7/site-packages/brainio_collection/lookup.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/brainio_base/assemblies.py:224: FutureWarning: The inplace argument has been deprecated and will be removed in a future version of xarray.\n",
      "  xr_data.set_index(append=True, inplace=True, **coords_d)\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "from sent_sampling.utils import extract_pool,model_grps_config\n",
    "import utils.optim_utils\n",
    "importlib.reload(utils.optim_utils)\n",
    "from sent_sampling.utils.optim_utils import optim, optim_pool, pt_create_corr_rdm_short, optim_group\n",
    "from sent_sampling.utils.data_utils import load_obj, SAVE_DIR, RESULTS_DIR, save_obj, ANALYZE_DIR\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.cm as cm \n",
    "from tqdm import tqdm\n",
    "import os \n",
    "# import tqdm.notebook as tq\n",
    "# from tqdm import tqdm_notebook\n",
    "# import fnmatch \n",
    "# import copy\n",
    "import pandas as pd \n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device =torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_precompute['grp_XY_corr_list'][1][0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(D_precompute_1['grp_XY_corr_list'][1][0,0,:])\n",
    "plt.plot(D_precompute['grp_XY_corr_list'][1][0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "loading /om/user/ehoseini/MyData/sent_sampling/gpt2-xl_ctrl_bert_gpt2_openaigpt_lm_1b_layers-dataset=coca_spok_filter_punct_10K_sample_1-activation-bench=None-ave=False_XY_corr_list.pkl\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8a3a2a401474>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mD_precompute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"gpt2-xl_ctrl_bert_gpt2_openaigpt_lm_1b_layers-dataset=coca_spok_filter_punct_10K_sample_{i+1}-activation-bench=None-ave=False_XY_corr_list.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mD_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_precompute\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'grp_XY_corr_list'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/om/user/ehoseini/sent_sampling/utils/data_utils.py\u001b[0m in \u001b[0;36mload_obj\u001b[0;34m(filename_, silent)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mfilename_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconstruct_stimuli_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstimuli_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstimuli_data_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/om/user/ehoseini/miniconda3/lib/python3.7/site-packages/torch/storage.py\u001b[0m in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_load_from_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "D_list=[]\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    D_precompute=load_obj(os.path.join(SAVE_DIR, f\"gpt2-xl_ctrl_bert_gpt2_openaigpt_lm_1b_layers-dataset=coca_spok_filter_punct_10K_sample_{i+1}-activation-bench=None-ave=False_XY_corr_list.pkl\"))\n",
    "    D_list.append(D_precompute['grp_XY_corr_list'][1][0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trues=[]\n",
    "for D in D_list:\n",
    "    truess=[]\n",
    "    for D1 in D_list:\n",
    "        print(all((D==D1).numpy().flatten()))\n",
    "        truess.append(all((D==D1).numpy().flatten()))\n",
    "    trues.append(truess) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim_ids=['coordinate_ascent_eh-obj=D_s-n_iter=1000-n_samples=50-n_init=1-run_gpu=True',\n",
    "#           'coordinate_ascent_eh-obj=D_s-n_iter=1000-n_samples=200-n_init=1-run_gpu=True',\n",
    "#            'coordinate_ascent_eh-obj=D_s-n_iter=1000-n_samples=250-n_init=1-run_gpu=True']\n",
    "optim_ids=['coordinate_ascent_eh-obj=D_s-n_iter=1000-n_samples=250-n_init=1-run_gpu=True']\n",
    "results_files=['results_gpt2-xl_ctrl_bert_gpt2_openaigpt_lm_1b_layers-dataset=coca_spok_filter_punct_10K_sample_1-activation-bench=None-ave=False_coordinate_ascent_eh-obj=D_s-n_iter=1000-n_samples=250-n_init=1-run_gpu=True_low_dim_gpu.pkl',\n",
    "               'results_gpt2-xl_ctrl_bert_gpt2_openaigpt_lm_1b_layers-dataset=coca_spok_filter_punct_10K_sample_2-activation-bench=None-ave=False_coordinate_ascent_eh-obj=D_s-n_iter=1000-n_samples=250-n_init=1-run_gpu=True_low_dim_gpu.pkl',\n",
    "               'results_gpt2-xl_ctrl_bert_gpt2_openaigpt_lm_1b_layers-dataset=coca_spok_filter_punct_10K_sample_3-activation-bench=None-ave=False_coordinate_ascent_eh-obj=D_s-n_iter=1000-n_samples=250-n_init=1-run_gpu=True_low_dim_gpu.pkl',\n",
    "'results_gpt2-xl_ctrl_bert_gpt2_openaigpt_lm_1b_layers-dataset=coca_spok_filter_punct_10K_sample_4-activation-bench=None-ave=False_coordinate_ascent_eh-obj=D_s-n_iter=1000-n_samples=250-n_init=1-run_gpu=True_low_dim_gpu.pkl',\n",
    "'results_gpt2-xl_ctrl_bert_gpt2_openaigpt_lm_1b_layers-dataset=coca_spok_filter_punct_10K_sample_5-activation-bench=None-ave=False_coordinate_ascent_eh-obj=D_s-n_iter=1000-n_samples=250-n_init=1-run_gpu=True_low_dim_gpu.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /om/user/ehoseini/MyData/sent_sampling/results/results_gpt2-xl_ctrl_bert_gpt2_openaigpt_lm_1b_layers-dataset=coca_spok_filter_punct_10K_sample_1-activation-bench=None-ave=False_coordinate_ascent_eh-obj=D_s-n_iter=1000-n_samples=250-n_init=1-run_gpu=True_low_dim_gpu.pkl\n",
      "loading /om/user/ehoseini/MyData/sent_sampling/results/results_gpt2-xl_ctrl_bert_gpt2_openaigpt_lm_1b_layers-dataset=coca_spok_filter_punct_10K_sample_2-activation-bench=None-ave=False_coordinate_ascent_eh-obj=D_s-n_iter=1000-n_samples=250-n_init=1-run_gpu=True_low_dim_gpu.pkl\n",
      "loading /om/user/ehoseini/MyData/sent_sampling/results/results_gpt2-xl_ctrl_bert_gpt2_openaigpt_lm_1b_layers-dataset=coca_spok_filter_punct_10K_sample_3-activation-bench=None-ave=False_coordinate_ascent_eh-obj=D_s-n_iter=1000-n_samples=250-n_init=1-run_gpu=True_low_dim_gpu.pkl\n",
      "loading /om/user/ehoseini/MyData/sent_sampling/results/results_gpt2-xl_ctrl_bert_gpt2_openaigpt_lm_1b_layers-dataset=coca_spok_filter_punct_10K_sample_4-activation-bench=None-ave=False_coordinate_ascent_eh-obj=D_s-n_iter=1000-n_samples=250-n_init=1-run_gpu=True_low_dim_gpu.pkl\n",
      "loading /om/user/ehoseini/MyData/sent_sampling/results/results_gpt2-xl_ctrl_bert_gpt2_openaigpt_lm_1b_layers-dataset=coca_spok_filter_punct_10K_sample_5-activation-bench=None-ave=False_coordinate_ascent_eh-obj=D_s-n_iter=1000-n_samples=250-n_init=1-run_gpu=True_low_dim_gpu.pkl\n"
     ]
    }
   ],
   "source": [
    "optim_files=[]\n",
    "optim_results=[]\n",
    "for result in results_files:\n",
    "        optim_file=os.path.join(RESULTS_DIR,result)\n",
    "        optim_files.append(optim_file)\n",
    "        optim_results.append(load_obj(optim_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1111it [00:00, 164642.33it/s]\n",
      "1111it [00:00, 87438.72it/s]\n",
      "1111it [00:00, 160480.48it/s]\n",
      "1111it [00:00, 165027.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group=gpt2-xl_layers-dataset=coca_spok_filter_punct_10K_sample_1-activation-bench=None-ave=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1111it [00:00, 132922.72it/s]\n",
      "1111it [00:00, 154423.11it/s]\n",
      "1111it [00:00, 163607.60it/s]\n",
      "1111it [00:00, 161829.20it/s]\n",
      "1112it [00:00, 114410.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group=gpt2-xl_layers-dataset=coca_spok_filter_punct_10K_sample_2-activation-bench=None-ave=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1111it [00:00, 135828.60it/s]\n",
      "1111it [00:00, 168573.30it/s]\n",
      "1111it [00:00, 165097.32it/s]\n",
      "1111it [00:00, 163006.67it/s]\n",
      "1111it [00:00, 165419.66it/s]\n",
      "1111it [00:00, 162978.17it/s]\n",
      "1111it [00:00, 160901.62it/s]\n",
      "1111it [00:00, 160996.12it/s]\n",
      "1112it [00:00, 161570.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group=gpt2-xl_layers-dataset=coca_spok_filter_punct_10K_sample_3-activation-bench=None-ave=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1111it [00:00, 137471.51it/s]\n",
      "1111it [00:00, 169099.38it/s]\n",
      "1111it [00:00, 166525.10it/s]\n",
      "1111it [00:00, 168214.27it/s]\n",
      "1111it [00:00, 124249.99it/s]\n",
      "1111it [00:00, 122251.80it/s]\n",
      "1111it [00:00, 163109.38it/s]\n",
      "1111it [00:00, 161694.43it/s]\n",
      "1112it [00:00, 160951.97it/s]\n",
      "1111it [00:00, 164178.27it/s]\n",
      "1111it [00:00, 133455.67it/s]\n",
      "1111it [00:00, 121724.88it/s]\n",
      "1111it [00:00, 122903.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group=gpt2-xl_layers-dataset=coca_spok_filter_punct_10K_sample_4-activation-bench=None-ave=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1111it [00:00, 104987.54it/s]\n",
      "1111it [00:00, 165278.84it/s]\n",
      "1111it [00:00, 167874.91it/s]\n",
      "1111it [00:00, 161168.74it/s]\n",
      "1112it [00:00, 127982.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group=gpt2-xl_layers-dataset=coca_spok_filter_punct_10K_sample_5-activation-bench=None-ave=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1111it [00:00, 135115.74it/s]\n",
      "1111it [00:00, 166145.10it/s]\n",
      "1111it [00:00, 164166.70it/s]\n",
      "1111it [00:00, 163510.01it/s]\n",
      "1111it [00:00, 162347.90it/s]\n",
      "1111it [00:00, 164636.51it/s]\n",
      "1111it [00:00, 165460.77it/s]\n",
      "1111it [00:00, 160751.75it/s]\n",
      "1112it [00:00, 162262.25it/s]\n"
     ]
    }
   ],
   "source": [
    "group_ids=[]\n",
    "optim_ids=[]\n",
    "results_o=[]\n",
    "results_sent=[]\n",
    "#resutls_rnd=[]\n",
    "#results_rand=[]\n",
    "extract_name='gpt2-xl_ctrl_bert_gpt2_openaigpt_lm_1b_layers'\n",
    "resutls_optim=[]\n",
    "optim_set=[]\n",
    "selected_sents=[]\n",
    "for idx, res in enumerate(optim_results):\n",
    "    extract_grp=res['extractor_grp_name']\n",
    "    \n",
    "    \n",
    "    \n",
    "    optimizer_obj = optim_pool[res['optimizatin_name']]()\n",
    "    \n",
    "    optim_group_obj = optim_group(n_init=optimizer_obj.n_init,\n",
    "                                  extract_group_name=extract_name,\n",
    "                                  ext_group_ids=extract_grp,\n",
    "                                  n_iter=optimizer_obj.n_iter,\n",
    "                                  N_s=optimizer_obj.N_s,\n",
    "                                  objective_function=optimizer_obj.objective_function,\n",
    "                                  optim_algorithm=optimizer_obj.optim_algorithm,\n",
    "                                  run_gpu=optimizer_obj.run_gpu)\n",
    "        \n",
    "    # get the sentences \n",
    "    print(f'{optim_group_obj.ext_group_ids[0]}')\n",
    "    ext_obj=extract_pool[optim_group_obj.ext_group_ids[0]]()\n",
    "    ext_obj.load_dataset()\n",
    "    sentences=[x['text'] for x in ext_obj.data_]\n",
    "    #optim_group_obj.load_extr_grp_and_corr_rdm_in_low_dim()\n",
    "    #D_s_rand=[]\n",
    "    #calculate rand \n",
    "    #for x in range(50):\n",
    "    #    optim_group_obj.gpu_obj_function(np.random.choice(optim_group_obj.N_S, size=optimizer_obj.N_s, replace=False))\n",
    "   #     D_s_rand.append(optim_group_obj.d_optim_list)\n",
    "   # D_s_ran=list(np.asarray(D_s_rand).transpose())\n",
    "   # results_rand.append(list(np.asarray(D_s_rand).transpose()))\n",
    "    # calculate optimized \n",
    "#    optim_group_obj.gpu_obj_function(res['optimized_S'])\n",
    "   # resutls_optim.append(optim_group_obj.d_optim_list)\n",
    "    optim_set.append(res['optimized_S'])\n",
    "    results_o.append(res['optimized_d'])\n",
    "    results_sent.append(sentences)\n",
    "    selected_sents.append([[idx+1,ext_obj.data_[x]['text']] for x in np.sort(res['optimized_S'])])\n",
    "\n",
    "    #for idx,ext_name in enumerate(res['extractor_grp_name']):\n",
    "        #group_ids.append(ext_name) \n",
    "        #optim_ids.append(res['optimizatin_name'])\n",
    "  #      results_o.append(optim_group_obj.d_optim_list[idx])\n",
    "\n",
    "#      resutls_rnd.append(D_s_ran[idx])\n",
    "        #optim_set.append(res['optimized_S'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_sents_flat=[item for sublist in selected_sents for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set_num</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Let's show everybody the restraint system righ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I want to talk about how much we bonded over b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Gee, what if he said no? That would have been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Weve got to do what our Republican colleagues ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Let's go to Al and get a check of the weather.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>5</td>\n",
       "      <td>The phone call I got was the \" meet the police...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>5</td>\n",
       "      <td>So maybe it's an idea that's catching on.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>5</td>\n",
       "      <td>And that perfect alignment will alter Earth's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>5</td>\n",
       "      <td>And very - in both Iowa and New Hampshire the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>5</td>\n",
       "      <td>We can show you what it looks like.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1250 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      set_num                                           sentence\n",
       "0           1  Let's show everybody the restraint system righ...\n",
       "1           1  I want to talk about how much we bonded over b...\n",
       "2           1  Gee, what if he said no? That would have been ...\n",
       "3           1  Weve got to do what our Republican colleagues ...\n",
       "4           1     Let's go to Al and get a check of the weather.\n",
       "...       ...                                                ...\n",
       "1245        5  The phone call I got was the \" meet the police...\n",
       "1246        5          So maybe it's an idea that's catching on.\n",
       "1247        5  And that perfect alignment will alter Earth's ...\n",
       "1248        5  And very - in both Iowa and New Hampshire the ...\n",
       "1249        5                We can show you what it looks like.\n",
       "\n",
       "[1250 rows x 2 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame(data=dict(set_num=[x[0] for x in selected_sents_flat],sentence=[x[1] for x in selected_sents_flat]))\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(RESULTS_DIR,f\"sentence_group=gpt2-xl_ctrl_bert_gpt2_openaigpt_lm_1b_layers-dataset=coca_spok_filter_punct_10K_sample_1_to_5-activation-bench=None-ave=False_1_coordinate_ascent_eh-obj=D_s.csv\"), 'w') as f:\n",
    "    df2.to_csv(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.86519027, 0.8602492, 0.860658, 0.8630843, 0.86337405]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=optim_results[1]\n",
    "extract_grp=res['extractor_grp_name']\n",
    "    \n",
    "    \n",
    "    \n",
    "optimizer_obj = optim_pool[res['optimizatin_name']]()\n",
    "    \n",
    "optim_group_obj = optim_group(n_init=optimizer_obj.n_init,\n",
    "                                  extract_group_name=extract_name,\n",
    "                                  ext_group_ids=extract_grp,\n",
    "                                  n_iter=optimizer_obj.n_iter,\n",
    "                                  N_s=optimizer_obj.N_s,\n",
    "                                  objective_function=optimizer_obj.objective_function,\n",
    "                                  optim_algorithm=optimizer_obj.optim_algorithm,\n",
    "                                  run_gpu=optimizer_obj.run_gpu)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "optim_group_obj.load_extr_grp_and_corr_rdm_in_low_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ext_obj=extract_pool[optim_group_obj.ext_group_ids[1]]()\n",
    "ext_obj.load_dataset()\n",
    "ext_obj()\n",
    "            # load optim\n",
    "optim_obj = optim(optim_algorithm=optim_group_obj.optim_algorithm, objective_function=optim_group_obj.objective_function,\n",
    "                          n_init=optim_group_obj.n_init, n_iter=optim_group_obj.n_iter, run_gpu=optim_group_obj.run_gpu, N_s=optim_group_obj.N_s)\n",
    "optim_group_obj.optim_obj.load_extractor(ext_obj)#\n",
    "del ext_obj\n",
    "optim_group_obj.N_S=optim_group_obj.optim_obj.N_S\n",
    "#optim_group_obj.optim_obj.precompute_corr_rdm_on_gpu(low_dim=200,low_resolution=True,cpu_dump=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_dict=optim_group_obj.optim_obj.activations[1]\n",
    "act_=[x[0] for x in act_dict['activations']]\n",
    "act = torch.tensor(act_, dtype=float, device=device,requires_grad=False)\n",
    "act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_=[x[0] if isinstance(act_dict['activations'][0], list) else x for x in act_dict['activations']]\n",
    "act = torch.tensor(act_, dtype=float, device=device,requires_grad=False)\n",
    "act.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_group_obj\n",
    "if os.path.exists(os.path.join(SAVE_DIR,f\"{optim_group_obj.extract_group_name}_XY_corr_list.pkl\")):\n",
    "        D_precompute=load_obj(os.path.join(SAVE_DIR, f\"{optim_group_obj.extract_group_name}_XY_corr_list.pkl\"))\n",
    "        optim_group_obj.grp_XY_corr_list=D_precompute['grp_XY_corr_list']\n",
    "        optim_group_obj.N_S=D_precompute['N_S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#sort_idx=np.asarray([x+optim_ids[idx] for idx,x in enumerate(group_ids)]).argsort()\n",
    "sort_idx=np.asarray([f\"{x}_{optim_ids[idx]}\" for idx,x in enumerate(group_ids)]).argsort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_groups=list(set(group_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[15,15])\n",
    "ax = fig.add_axes([.1,.1,.4,.6])\n",
    "\n",
    "cmap=cm.get_cmap('viridis_r')\n",
    "\n",
    "alph_col=cmap(np.divide(range(len(unique_groups)),len(unique_groups)))\n",
    "tick_l=[]\n",
    "tick=[]\n",
    "for idx,sort_idx in enumerate(sort_idx):\n",
    "    res=results_o[sort_idx]\n",
    "    \n",
    "    ax.barh(idx,res,height=0.4,color=alph_col[[unique_groups.index(group_ids[sort_idx])],:],alpha=.9,edgecolor=(0,0,0),linewidth=2,label=optim_ids[sort_idx])\n",
    "    \n",
    "    str_val=\"{:.5f}\".format(res)\n",
    "    print(f\"{str_val}\")\n",
    "    optim_type=re.search('obj=\\w+-',optim_ids[sort_idx])[0][0:-1]\n",
    "    num_s=re.search('n_samples=\\d+-',optim_ids[sort_idx])[0][0:-1]\n",
    "    group_type=re.search('group=.+-d',group_ids[sort_idx])[0][0:-2]\n",
    "    dataset_type=re.search('dataset=\\w+-',group_ids[sort_idx])[0][0:-1]\n",
    "    tick_l.append(f\" {group_type}, {optim_type}, s: {num_s} \\n {dataset_type}  ,  value:{str_val}\")\n",
    "    tick.append(idx)\n",
    "\n",
    "    D_s_rand=resutls_rnd[sort_idx]\n",
    "    ax.barh(idx, np.mean(D_s_rand),height=0.4, align='center',color=(1,1,1),alpha=.7,edgecolor=(1,1,1),linewidth=2)\n",
    "    ax.errorbar(np.mean(D_s_rand),idx,xerr=np.std(D_s_rand),color=(0,0,0),label='random set',linewidth=3)\n",
    "\n",
    "\n",
    "#ax.set_xlabel(f\"D_s \\n\\n  models:{ext_obj.model_spec}\\nlayers:{ext_obj.layer_spec} averaging : {ext_obj.average_sentence}\",fontsize=12)\n",
    "ax.set_yticklabels(tick_l,fontsize=12)\n",
    "ax.set_yticks(tick)\n",
    "#ax.set_title(res['optimizatin_name'],fontsize=12)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "#ax.legend(bbox_to _anchor=(1.1, .85), frameon=True,fontsize=12)\n",
    "ax.invert_yaxis()\n",
    "ax.grid()\n",
    "#fig.savefig(os.path.join(Analysis_path,'DV_test_gamma_alpha_is_0.pdf'))\n",
    "#plt.savefig(os.path.join(ANALYZE_DIR,f\"U01_SET2_optimization_results.png\"), dpi=None, facecolor='w', edgecolor='w',\n",
    "#       orientation='portrait',transparent=True, bbox_inches=None, pad_inches=0.1,frameon=False)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_groups.index(group_ids[sort_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter=50\n",
    "\n",
    "for id_g, res in enumerate(group_ids):\n",
    "    if id_g>-1:\n",
    "        \n",
    "        ext_obj=extract_pool[res]()\n",
    "        mdl_name=np.unique(ext_obj.model_spec)[0]\n",
    "        layers_to_optim=ext_obj.layer_spec\n",
    "        group=f'{mdl_name}_layers'\n",
    "        extractor_id=f'group={group}-dataset={ext_obj.dataset}-{ext_obj.extract_type}-bench=None-ave={ext_obj.average_sentence}'\n",
    "        extractor_obj = extract_pool[extractor_id]()\n",
    "        extractor_obj.load_dataset()\n",
    "        model_layers = extractor_obj.layer_name\n",
    "        extractor_obj()\n",
    "        mdl_name=str(np.unique(extractor_obj.model_spec).squeeze())\n",
    "        optim_obj=optim_pool[optim_ids[id_g]]()\n",
    "        optim_obj.load_extractor(extractor_obj)\n",
    "        layer_id_list=[x['layer'] for x in optim_obj.activations]\n",
    "        del extractor_obj\n",
    "        activation_list=[]\n",
    "        var_explained=[]\n",
    "        loadings=[]\n",
    "        components=[]\n",
    "        pca_type='fixed'\n",
    "        for idx, act_dict in tqdm(enumerate(optim_obj.activations)):\n",
    "            act=torch.tensor(act_dict['activations'], dtype=float, device=optim_obj.device, requires_grad=False)\n",
    "    # act must be in m sample * n feature shape ,\n",
    "            u,s,v=torch.pca_lowrank(act,q=200)\n",
    "    # keep 85% variance explained ,\n",
    "            idx_85=torch.cumsum(s**2,dim=0)/torch.sum(s**2)<.85\n",
    "            cols=list(torch.where(idx_85)[0].cpu().numpy())\n",
    "            if pca_type=='fixed':\n",
    "                act_pca = torch.matmul(act, v[:, :100])\n",
    "            elif pca_type=='equal_var':\n",
    "                act_pca = torch.matmul(act, v[:, cols])\n",
    "        \n",
    "            activation_list.append(act_pca)\n",
    "            var_explained.append(torch.cumsum(torch.cat((torch.tensor([0],device=optim_obj.device),s**2)),dim=0)/torch.sum(s**2))\n",
    "    #var_explained.append(torch.cumsum(s**2,dim=0)/torch.sum(s**2))\n",
    "        var_explained=torch.stack(var_explained).cpu()\n",
    "  \n",
    "        total_sent=activation_list[0].shape[0]\n",
    "        num_samples=len(optim_set[id_g])\n",
    "        act_list_norm=[(X-X.mean(axis=1,keepdim=True)) for X in activation_list]\n",
    "        act_list_norm=[torch.nn.functional.normalize(X) for X in act_list_norm]\n",
    "        layer_dist=[]\n",
    "        for idx in tqdm_notebook(range(len(activation_list))):\n",
    "            pair_dist=[]\n",
    "            for idy in tqdm_notebook(range(len(activation_list)),position=1):\n",
    "                sample_dist=[]\n",
    "                pair_list_norm=[act_list_norm[idx],act_list_norm[idy]]\n",
    "                XY_corr_list = [torch.tensor(1, device=X.device, dtype=float) - torch.mm(X, torch.transpose(X, 1, 0)) for X in\n",
    "                            pair_list_norm]\n",
    "                for sample_iter in range(num_iter):\n",
    "                    samples=torch.tensor(np.random.choice(total_sent,num_samples,replace=False), dtype = torch.long, device = act_list_norm[0].device)\n",
    "                    pairs = torch.combinations(samples, with_replacement=False)\n",
    "                    XY_corr_sample = [XY_corr[pairs[:, 0], pairs[:, 1]] for XY_corr in XY_corr_list]\n",
    "                    XY_corr_sample_tensor = torch.stack(XY_corr_sample)\n",
    "                    XY_corr_sample_tensor = torch.transpose(XY_corr_sample_tensor, 1, 0)\n",
    "                    if XY_corr_sample_tensor.shape[1] < XY_corr_sample_tensor.shape[0]:\n",
    "                        XY_corr_sample_tensor = torch.transpose(XY_corr_sample_tensor, 1, 0)\n",
    "                    assert (XY_corr_sample_tensor.shape[1] > XY_corr_sample_tensor.shape[0])\n",
    "                    d_mat = pt_create_corr_rdm_short(XY_corr_sample_tensor, device=samples.device)\n",
    "                #n1 = d_mat.shape[1],\n",
    "                #correction = n1 * n1 / (n1 * (n1 - 1) / 2),\n",
    "                #d_val = correction * d_mat.mean(dim=(0, 1)),\n",
    "                    d_val = d_mat[0,1]\n",
    "                    sample_dist.append(d_val)\n",
    "                pair_dist.append(torch.stack(sample_dist))\n",
    "            layer_dist.append(pair_dist)\n",
    "            \n",
    "        optim_act_list_norm=[x[optim_set[id_g],:] for x in act_list_norm]\n",
    "        layer_similarity=[pt_create_corr_rdm_short(x) for x in optim_act_list_norm]\n",
    "        optim_pairs = torch.combinations(torch.tensor(np.arange(len(optim_set[id_g]))), with_replacement=False)\n",
    "        layer_optim_dist=[]\n",
    "        for idx in tqdm_notebook(range(len(activation_list))):\n",
    "            pair_optim_dist=[]\n",
    "            for idy in tqdm_notebook(range(len(activation_list)),position=1):\n",
    "                pair_similarity=[layer_similarity[idx],layer_similarity[idy]]\n",
    "                XY_corr_sample = [XY_corr[optim_pairs[:, 0], optim_pairs[:, 1]] for XY_corr in pair_similarity]\n",
    "                XY_corr_sample_tensor=torch.stack(XY_corr_sample)\n",
    "                d_mat = pt_create_corr_rdm_short(XY_corr_sample_tensor, device=XY_corr_sample_tensor.device)\n",
    "                d_val =d_mat[0,1].cpu()\n",
    "                pair_optim_dist.append([d_val])\n",
    "            layer_optim_dist.append(pair_optim_dist)\n",
    "        print(\"Done!\")\n",
    "        pereira_settings=extract_pool['group=best_performing_pereira_1-dataset=ud_sentences-activation-bench=None-ave=False']()\n",
    "        scores = pd.read_csv(os.path.join(SAVE_DIR, 'scoresscoresscores', 'scores-Pereira2018-encoding-normalized.csv'))\n",
    "        score_layer = list(scores['layer'][scores['model'] == mdl_name])\n",
    "        score_benchmark = list(scores['benchmark'][scores['model'] == mdl_name])\n",
    "        score_score = np.asarray(scores['score'][scores['model'] == mdl_name])\n",
    "        score_error = np.asarray(scores['error'][scores['model'] == mdl_name])\n",
    "        try :\n",
    "            model_loc = pereira_settings.model_spec.index(mdl_name)\n",
    "            pereira_layer_id=pereira_settings.layer_spec[model_loc]\n",
    "        except ValueError as e:\n",
    "            pereira_layer_id=np.argmax(score_score)\n",
    "        \n",
    "        \n",
    "        Pereira_dist=torch.mean(torch.stack(layer_dist[pereira_layer_id]),dim=1)\n",
    "        dist_val,dist_idx=torch.sort(Pereira_dist)\n",
    "        assert(dist_idx[0]==pereira_layer_id)\n",
    "        cuts=np.linspace(dist_val.cpu().numpy().min(),dist_val.cpu().numpy().max(),4,endpoint=False)\n",
    "        Pereira_dist_optim=torch.tensor([torch.stack([x[0] for x in y]) for y in layer_optim_dist][pereira_layer_id])\n",
    "        dist_val_optim,dist_idx_optim=torch.sort(Pereira_dist_optim)\n",
    "        Pereira_ordered=Pereira_dist_optim[dist_idx]\n",
    "        assert(dist_idx[0]==pereira_layer_id)\n",
    "        high_val=float(torch.max(torch.stack([torch.stack([x[0] for x in y]) for y in layer_optim_dist]).cpu()))\n",
    "        fig = plt.figure(figsize=(8,8*1.5))\n",
    "        ax = fig.add_axes((.1,.4,.25*1.5,.25))\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes('right', size=.05, pad=0.1)\n",
    "        im = ax.imshow(torch.stack([torch.stack([x.mean() for x in y]) for y in layer_dist]).cpu(),vmin=0,vmax=high_val,aspect='auto',interpolation='none')\n",
    "        ax.set_yticks(np.arange(var_explained.shape[0]))\n",
    "        ax.set_xticks(np.arange(var_explained.shape[0]))\n",
    "        ax.set_yticklabels([f\" {model_layers[idx]}\" for idx,x in enumerate(var_explained) ],fontsize=8)\n",
    "        ax.set_xticklabels([f\"{model_layers[idx]}\" for idx,x in enumerate(var_explained) ],rotation=90,fontsize=8)\n",
    "\n",
    "        cbar = fig.colorbar(im, cax=cax)\n",
    "\n",
    "\n",
    "        ax = fig.add_axes((.55,.4,.25*1.5,.25))\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes('right', size=.05, pad=0.1)\n",
    "        im = ax.imshow(torch.stack([torch.stack([x[0] for x in y]) for y in layer_optim_dist]).cpu(),aspect='auto',vmin=0,vmax=high_val,interpolation='none')\n",
    "        ax.set_yticks(np.arange(var_explained.shape[0]))\n",
    "        ax.set_xticks(np.arange(var_explained.shape[0]))\n",
    "        ax.set_yticklabels([ ])\n",
    "        ax.set_xticklabels([ ],rotation=90)\n",
    "\n",
    "        cbar = fig.colorbar(im, cax=cax)\n",
    "        ax = fig.add_axes((.2,.1,.7,.12))\n",
    "        ax.scatter(np.arange(dist_val.cpu().shape[0]),dist_val.cpu(),zorder=3)\n",
    "        ax.scatter(np.arange(dist_val_optim.cpu().shape[0]),Pereira_ordered.cpu(),zorder=4)\n",
    "\n",
    "        ax.set_xlim((-1,dist_val_optim.cpu().shape[0]+1))\n",
    "        ax.set_ylim((0-.05,np.max(dist_val_optim.cpu().numpy())+.05))\n",
    "        #[ax.plot(plt.xlim(),[x,x],'k--') for x in cuts],\n",
    "        #closest_points=[np.argmin(np.abs(dist_val.cpu()-x)) for x in cuts],\n",
    "        #optimized_dist=torch.tensor([dist_val_optim[dist_idx_optim==x] for x in extractor_obj.layer_spec]),\n",
    "        #optimized_dist_loc=torch.tensor([torch.where(dist_idx_optim==x) for x in extractor_obj.layer_spec]),\n",
    "        #ax.scatter(optimized_dist_loc.cpu(),optimized_dist.cpu(),50,color=(0,0,0)),\n",
    "        closest_points=[int(np.where(dist_idx.cpu().numpy()==x)[0]) for x in layers_to_optim]\n",
    "        [ax.scatter(x,dist_val[x].cpu().numpy(),50,color=(0,0,0),zorder=5) for x in closest_points]\n",
    "        [ax.scatter(x,Pereira_ordered[x].cpu().numpy(),50,color=(1,0,0),zorder=5) for x in closest_points]\n",
    "\n",
    "        ax.set_xticks(tuple(np.arange(dist_val.cpu().shape[0])))\n",
    "        ax.set_xticklabels(dist_idx.cpu().numpy(),fontsize=8)\n",
    "        ax.set_xticklabels([model_layers[int(x)] for x in dist_idx.cpu().numpy()],rotation=90,fontsize=8)\n",
    "        [ax.plot([x,x],plt.ylim(),'k-',zorder=2) for x in closest_points]\n",
    "        \n",
    "        ax.set_title( f\"{res} , \\n {optim_ids[id_g]}\",fontsize=10)\n",
    "        \n",
    "        #plt.savefig(os.path.join(ANALYZE_DIR,f\"{res['extractor_name']}_{res['optimizatin_name']}_RDM.png\"), dpi=None, facecolor='w', edgecolor='w',\n",
    "       #orientation='landscape',transparent=True, bbox_inches=None, pad_inches=0.1,frameon=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, res in enumerate(optim_results):\n",
    "    select_sent=[]\n",
    "    values=[]\n",
    "    for name in res['extractor_grp_name']:\n",
    "        ext_obj=extract_pool[name]()\n",
    "        ext_obj.load_dataset()\n",
    "        [values.append([id, ext_obj.data_[id]['text']]) for id in np.sort(res['optimized_S'])]\n",
    "        with open(os.path.join(RESULTS_DIR,f\"sentences_{name}_{res['optimizatin_name']}.txt\"), 'w') as f:\n",
    "            for item in values:\n",
    "                f.write(\"%d, %s\\n\" % (item[0],item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(optim_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pca plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_g, res in enumerate(group_ids):\n",
    "    if id_g>-1:\n",
    "        ext_obj=extract_pool[res]()\n",
    "        mdl_name=np.unique(ext_obj.model_spec)[0]\n",
    "        layers_to_optim=ext_obj.layer_spec\n",
    "        group=f'{mdl_name}_layers'\n",
    "        extractor_id=f'group={group}-dataset={ext_obj.dataset}-{ext_obj.extract_type}-bench=None-ave={ext_obj.average_sentence}'\n",
    "        extractor_obj = extract_pool[extractor_id]()\n",
    "        extractor_obj.load_dataset()\n",
    "        model_layers = extractor_obj.layer_name\n",
    "        extractor_obj()\n",
    "        mdl_name=str(np.unique(extractor_obj.model_spec).squeeze())\n",
    "        optim_obj=optim_pool[optim_ids[id_g]]()\n",
    "        optim_obj.load_extractor(extractor_obj)\n",
    "        layer_id_list=[x['layer'] for x in optim_obj.activations]\n",
    "        del extractor_obj\n",
    "        activation_list=[]\n",
    "        var_explained=[]\n",
    "        loadings=[]\n",
    "        components=[]\n",
    "        pca_type='fixed'\n",
    "        for idx, act_dict in tqdm(enumerate(optim_obj.activations)):\n",
    "            act=torch.tensor(act_dict['activations'], dtype=float, device=optim_obj.device, requires_grad=False)\n",
    "    # act must be in m sample * n feature shape ,\n",
    "            u,s,v=torch.pca_lowrank(act,q=200)\n",
    "            loadings.append(torch.matmul(u,torch.diag(s)))\n",
    "            components.append(v)\n",
    "            # keep 85% variance explained ,\n",
    "            idx_85=torch.cumsum(s**2,dim=0)/torch.sum(s**2)<.85\n",
    "            cols=list(torch.where(idx_85)[0].cpu().numpy())\n",
    "            if pca_type=='fixed':\n",
    "                act_pca = torch.matmul(act, v[:, :100])\n",
    "            elif pca_type=='equal_var':\n",
    "                act_pca = torch.matmul(act, v[:, cols])\n",
    "        \n",
    "            activation_list.append(act_pca)\n",
    "            var_explained.append(torch.cumsum(torch.cat((torch.tensor([0],device=optim_obj.device),s**2)),dim=0)/torch.sum(s**2))\n",
    "    #var_explained.append(torch.cumsum(s**2,dim=0)/torch.sum(s**2))\n",
    "        var_explained=torch.stack(var_explained).cpu()\n",
    "        loadings_p12=[x[:,0:2].cpu() for x in loadings]\n",
    "        loadings_p12_norm=[x/torch.norm(x,dim=1,keepdim=True) for x in loadings_p12]\n",
    "        loadings_p12_len=[1e1*torch.norm(x,dim=1,keepdim=True) for x in loadings_p12]\n",
    "        rot_list=[]\n",
    "        all_angle_fixed=[]\n",
    "        for idx, load_norm in enumerate(loadings_p12_norm):\n",
    "            angle=np.arccos(load_norm[:,0].numpy())\n",
    "            y_cos=load_norm[:,1].numpy()\n",
    "            angle_fixed=[angle[idy] if y > 0 else np.pi*2-angle[idy] for idy,y in enumerate(y_cos)]\n",
    "            mag_angle=loadings_p12_len[idx].squeeze()+1e3*torch.tensor(angle_fixed)\n",
    "            all_angle_fixed.append(angle_fixed)\n",
    "            rot=np.argsort(angle_fixed)\n",
    "            #rot=np.argsort(mag_angle)\n",
    "            rot_list.append(rot)\n",
    "        pereira_settings=extract_pool['group=best_performing_pereira_1-dataset=ud_sentences-activation-bench=None-ave=False']()\n",
    "        scores = pd.read_csv(os.path.join(SAVE_DIR, 'scoresscoresscores', 'scores-Pereira2018-encoding-normalized.csv'))\n",
    "        score_layer = list(scores['layer'][scores['model'] == mdl_name])\n",
    "        score_benchmark = list(scores['benchmark'][scores['model'] == mdl_name])\n",
    "        score_score = np.asarray(scores['score'][scores['model'] == mdl_name])\n",
    "        score_error = np.asarray(scores['error'][scores['model'] == mdl_name])\n",
    "        try :\n",
    "            model_loc = pereira_settings.model_spec.index(mdl_name)\n",
    "            pereira_layer_id=pereira_settings.layer_spec[model_loc]\n",
    "        except ValueError as e:\n",
    "            pereira_layer_id=np.argmax(score_score)\n",
    "        idx=pereira_layer_id\n",
    "        fig=plt.figure(figsize=(20,20))\n",
    "        num_colors=int(rot.shape[0])\n",
    "        h0=sns.color_palette(\"hls\", num_colors,as_cmap=True)\n",
    "        line_cols=np.flipud(h0(np.arange(num_colors)/num_colors))\n",
    "        #plt.suptitle(f\"{group}\\n{name}\",fontsize=16)\n",
    "        plot_ev_select=True\n",
    "        ax=plt.subplot(1,1,1)\n",
    "        l_v=loadings_p12[idx]\n",
    "        onehot_target=torch.nn.functional.one_hot(torch.tensor(rot_list[idx]))\n",
    "        val,indicies=torch.max(onehot_target.transpose(1,0),dim=1)\n",
    "        col_order=line_cols[indicies,:]\n",
    "        if plot_ev_select:\n",
    "            scatter1=ax.scatter(l_v[optim_set[id_g],0],l_v[optim_set[id_g],1],s=25,c='none',edgecolors=(0,0,0),linewidth=1.5,alpha=1,zorder=4)\n",
    "        scatter_pca=ax.scatter(l_v[:,0],l_v[:,1],s=20,c=col_order,edgecolors=(1,1,1),alpha=.5,linewidth=.5,zorder=3)\n",
    "        right_side = ax.spines[\"right\"]\n",
    "        right_side. set_visible(False)\n",
    "        right_side = ax.spines[\"top\"]\n",
    "        right_side. set_visible(False)\n",
    "        right_side = ax.spines[\"left\"]\n",
    "        right_side. set_visible(False)\n",
    "        right_side = ax.spines[\"bottom\"]\n",
    "        right_side. set_visible(False)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        #ax.set_xlim(ax_lims[:,0])\n",
    "        #ax.set_ylim(ax_lims[:,1])\n",
    "        ax.plot(ax.get_xlim(),[0,0],'-',zorder=0,linewidth=1,color=(.5,.5,.5))\n",
    "        ax.plot([0,0],ax.get_ylim(),'-',zorder=0,linewidth=1,color=(.5,.5,.5))\n",
    "#        ax.set_title(f\"{optimizer_obj.activations[idx]['model_name']}, Layer {optimizer_obj.activations[idx]['layer']}\",fontsize=20)\n",
    "        ax.set_title( f\"{res} , \\n {optim_ids[id_g]}\",fontsize=10)\n",
    "        ax.spines['left'].set_position(('axes', 0.00))\n",
    "        ax.spines['left'].set_smart_bounds(True)\n",
    "            #ax.yaxis.set_ticks_position('left')   \n",
    "        ax.spines['left'].set_position(('axes', 0.00))\n",
    "            # ax.spines['left'].set_smart_bounds(True)\n",
    "        ax.yaxis.set_ticks_position('left')\n",
    "\n",
    "        ax.spines['bottom'].set_position(('axes', 0.00))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT RDMS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter=100\n",
    "num_samples=50\n",
    "\n",
    "for idx, res in enumerate(optim_results):\n",
    "    ext_obj=extract_pool[res['extractor_name']]()\n",
    "    mdl_name=np.unique(ext_obj.model_spec)[0]\n",
    "    group=f'{mdl_name}_layers'\n",
    "    extractor_id=f'group={group}-dataset={ext_obj.dataset}-{ext_obj.extract_type}-bench=None-ave={ext_obj.average_sentence}'\n",
    "    extractor_obj = extract_pool[extractor_id]()\n",
    "    extractor_obj.load_dataset()\n",
    "    model_layers = extractor_obj.layer_name\n",
    "    extractor_obj()\n",
    "    mdl_name=str(np.unique(extractor_obj.model_spec).squeeze())\n",
    "    optim_obj=optim_pool[res['optimizatin_name']]()\n",
    "    optim_obj.load_extractor(extractor_obj)\n",
    "    layer_id_list=[x['layer'] for x in optim_obj.activations]\n",
    "    del extractor_obj\n",
    "    activation_list=[]\n",
    "    var_explained=[]\n",
    "    loadings=[]\n",
    "    components=[]\n",
    "    pca_type='fixed'\n",
    "    for idx, act_dict in tqdm(enumerate(optim_obj.activations)):\n",
    "        act=torch.tensor(act_dict['activations'], dtype=float, device=optim_obj.device, requires_grad=False)\n",
    "# act must be in m sample * n feature shape ,\n",
    "        u,s,v=torch.pca_lowrank(act,q=200)\n",
    "# keep 85% variance explained ,\n",
    "        idx_85=torch.cumsum(s**2,dim=0)/torch.sum(s**2)<.85\n",
    "        cols=list(torch.where(idx_85)[0].cpu().numpy())\n",
    "        if pca_type=='fixed':\n",
    "            act_pca = torch.matmul(act, v[:, :100])\n",
    "        elif pca_type=='equal_var':\n",
    "            act_pca = torch.matmul(act, v[:, cols])\n",
    "\n",
    "        activation_list.append(act_pca)\n",
    "        var_explained.append(torch.cumsum(torch.cat((torch.tensor([0],device=optim_obj.device),s**2)),dim=0)/torch.sum(s**2))\n",
    "# #var_explained.append(torch.cumsum(s**2,dim=0)/torch.sum(s**2))\n",
    "#     var_explained=torch.stack(var_explained).cpu()\n",
    "\n",
    "    total_sent=activation_list[0].shape[0]\n",
    "    act_list_norm=[(X-X.mean(axis=1,keepdim=True)) for X in activation_list]\n",
    "    act_list_norm=[torch.nn.functional.normalize(X) for X in act_list_norm]\n",
    "    optm_activation=[act_list_norm[x] for x in res['layer_spec']]\n",
    "    layer_rand_dist=[]\n",
    "    layer_optim_dist=[]\n",
    "    for idx in tqdm(range(len(optm_activation))):\n",
    "        pair_list_norm=[optm_activation[idx]]\n",
    "        XY_corr_list = [torch.tensor(1, device=X.device, dtype=float) - torch.mm(X, torch.transpose(X, 1, 0)) for X in                         \n",
    "        pair_list_norm]\n",
    "        corr_samples=[]\n",
    "        for sample_iter in range(num_iter):\n",
    "            samples=torch.tensor(np.random.choice(total_sent,num_samples,replace=False), dtype = torch.long, device = act_list_norm[0].device)\n",
    "            XY_corr_sample=[XY_corr[samples, :] for XY_corr in XY_corr_list]\n",
    "            XY_corr_sample = [XY_corr[:, samples] for XY_corr in XY_corr_sample]\n",
    "            corr_samples.append(XY_corr_sample[0])\n",
    "        layer_rand_dist.append(corr_samples)\n",
    "        \n",
    "        \n",
    "        XY_corr_optim=[XY_corr[res['optimized_S'], :] for XY_corr in XY_corr_list]\n",
    "        XY_corr_optim = [XY_corr[:, res['optimized_S']] for XY_corr in XY_corr_optim]\n",
    "        layer_optim_dist.append(XY_corr_optim)\n",
    "\n",
    "    min_val=0\n",
    "    max_val=torch.stack([x[0] for x in layer_optim_dist]).max()\n",
    "    rand_dist_mean=[torch.stack(rand_dist).mean(axis=0) for rand_dist in layer_rand_dist]\n",
    "    max_val_1=torch.stack(rand_dist_mean).max()\n",
    "    max_val=np.max([max_val,max_val_1])\n",
    "    fig,ax = plt.subplots(2,len(layer_optim_dist),figsize=(8*len(layer_optim_dist)/2,8) )\n",
    "    for idx,_ in enumerate(layer_rand_dist):\n",
    "        rand_dist=layer_rand_dist[idx]\n",
    "        #ax[0,idx].imshow(torch.stack(rand_dist).mean(axis=0).cpu(),vmin=0,vmax=max_val)\n",
    "        ax[0,idx].imshow(rand_dist[0].cpu(),vmin=0,vmax=max_val)\n",
    "        ax[0,idx].set_title(model_layers[res['layer_spec'][idx]])\n",
    "    ax[0,0].set_ylabel('random',fontsize=16)\n",
    "    ax[0,0].set_xlabel('sentence')\n",
    "    for idx,_ in enumerate(layer_optim_dist):\n",
    "        optim_dist=layer_optim_dist[idx]\n",
    "        ax[1,idx].imshow(optim_dist[0].cpu(),vmin=0,vmax=max_val)\n",
    "\n",
    "    ax[1,0].set_ylabel('optimized',fontsize=16)\n",
    "    plt.suptitle(f\"{res['extractor_name']} , \\n {res['optimizatin_name']} \\n\",fontsize=16)\n",
    "\n",
    "#     optim_act_list_norm=[x[res['optimized_S'],:] for x in act_list_norm]\n",
    "#     layer_similarity=[pt_create_corr_rdm_short(x) for x in optim_act_list_norm]\n",
    "#     optim_pairs = torch.combinations(torch.tensor(np.arange(len(res['optimized_S']))), with_replacement=False)\n",
    "#     layer_optim_dist=[]\n",
    "#     for idx in tqdm_notebook(range(len(activation_list))):\n",
    "#         pair_optim_dist=[]\n",
    "#         for idy in tqdm_notebook(range(len(activation_list)),position=1):\n",
    "#             pair_similarity=[layer_similarity[idx],layer_similarity[idy]]\n",
    "#             XY_corr_sample = [XY_corr[optim_pairs[:, 0], optim_pairs[:, 1]] for XY_corr in pair_similarity]\n",
    "#             XY_corr_sample_tensor=torch.stack(XY_corr_sample)\n",
    "#             d_mat = pt_create_corr_rdm_short(XY_corr_sample_tensor, device=XY_corr_sample_tensor.device)\n",
    "#             d_val =d_mat[0,1].cpu()\n",
    "#             pair_optim_dist.append([d_val])\n",
    "#         layer_optim_dist.append(pair_optim_dist)\n",
    "#     print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(optm_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(layer_rand_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_ids=['coordinate_ascent_eh-obj=D_s-n_iter=1000-n_samples=200-n_init=1-run_gpu=True']\n",
    "extract_id = ['group=gpt2_layers-dataset=coca_spok_filter_punct_10K_sample_1-activation-bench=None-ave=False',\n",
    "                  'group=openaigpt_layers-dataset=coca_spok_filter_punct_10K_sample_1-activation-bench=None-ave=False']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimizer_obj = optim_pool[optim_ids[0]]()\n",
    "optim_group_obj = optim_group(n_init=optimizer_obj.n_init,\n",
    "                                  ext_group_ids=extract_id,\n",
    "                                  n_iter=optimizer_obj.n_iter,\n",
    "                                  N_s=optimizer_obj.N_s,\n",
    "                                  objective_function=optimizer_obj.objective_function,\n",
    "                                  optim_algorithm=optimizer_obj.optim_algorithm,\n",
    "                                  run_gpu=optimizer_obj.run_gpu)\n",
    "optim_group_obj.load_extr_grp_and_corr_rdm_in_low_dim()\n",
    "\n",
    "    #calculate rand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_group_obj.grp_XY_corr_list[0][1].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device =torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit optim_group_obj.gpu_obj_function(np.random.choice(optim_group_obj.N_S, size=optimizer_obj.N_s, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XY_corr_obj_func(S,XY_corr_list):\n",
    "    device=XY_corr_list[0].device\n",
    "    samples = torch.tensor(S, dtype=torch.long, device=device)\n",
    "    pairs = torch.combinations(samples, with_replacement=False)\n",
    "    XY_corr_sample = [XY_corr[pairs[:, 0], pairs[:, 1]] for XY_corr in XY_corr_list]\n",
    "    XY_corr_sample_tensor = torch.stack(XY_corr_sample).float()\n",
    "    #XY_corr_sample_tensor = torch.transpose(XY_corr_sample_tensor, 1, 0)\n",
    "    #if XY_corr_sample_tensor.shape[1] < XY_corr_sample_tensor.shape[0]:\n",
    "    #    XY_corr_sample_tensor = torch.transpose(XY_corr_sample_tensor, 1, 0)\n",
    "    assert (XY_corr_sample_tensor.shape[1] > XY_corr_sample_tensor.shape[0])\n",
    "    d_mat = pt_create_corr_rdm_short(XY_corr_sample_tensor, device=XY_corr_list[0].device)\n",
    "    n1 = d_mat.shape[1]\n",
    "    correction = n1 * n1 / (n1 * (n1 - 1) / 2)\n",
    "    d_val = correction * d_mat.mean(dim=(0, 1))\n",
    "    d_val_mean = d_val.cpu().numpy().mean()\n",
    "    # do a version with std reductions too\n",
    "    mdl_pairs = torch.combinations(torch.tensor(np.arange(d_mat.shape[0])), with_replacement=False)\n",
    "    d_val_std = torch.std(d_mat[mdl_pairs[:, 0], mdl_pairs[:, 1]]).cpu().numpy()\n",
    "    d_optim = d_val_mean # - .2 * d_val_std\n",
    "    del XY_corr_list\n",
    "    return d_optim\n",
    "\n",
    "\n",
    "def cpu_obj_function(grp_XY_corr_list,S):\n",
    "    d_optim_list=[]\n",
    "    for XY_corr_list in grp_XY_corr_list:\n",
    "        d_optim_list.append(XY_corr_obj_func(S,XY_corr_list=XY_corr_list))\n",
    "    d_optim=np.mean(d_optim_list)\n",
    "    return d_optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(optim_group_obj.grp_XY_corr_list[1])\n",
    "gropu=[]\n",
    "for XY_corr_list in optim_group_obj.grp_XY_corr_list:\n",
    "    gropu.append([x.to(device) for x in XY_corr_list])\n",
    "#XY_corr_list = [x.to(device) for x in XY_corr_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit cpu_obj_function(optim_group_obj.grp_XY_corr_list,np.random.choice(optim_group_obj.N_S, size=optimizer_obj.N_s, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit cpu_obj_function(gropu,np.random.choice(optim_group_obj.N_S, size=optimizer_obj.N_s, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gpu_setup(optim_group,device):\n",
    "    for _,XY_corr_list in tqdm(enumerate(optim_group.grp_XY_corr_list)):\n",
    "        XY_corr_list_gpu = [x.to(device) for x in XY_corr_list]\n",
    "        optim_group.d_optim_list.append(optim_group.XY_corr_obj_func(np.random.choice(optim_group_obj.N_S, size=optimizer_obj.N_s, replace=False),XY_corr_list=XY_corr_list_gpu))\n",
    "        del XY_corr_list_gpu, XY_corr_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit test_gpu_setup(optim_group_obj,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for XY_corr_list in tqdm(enumerate(optim_group_obj.grp_XY_corr_list)):\n",
    "        XY_corr_list_gpu = [x.to(device) for x in XY_corr_list]\n",
    "        optim_group.d_optim_list.append(optim_group.XY_corr_obj_func(np.random.choice(optim_group_obj.N_S, size=optimizer_obj.N_s, replace=False),XY_corr_list=XY_corr_list_gpu))\n",
    "        del XY_corr_list_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_corr_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_corr_list_gpu = [x.to(device) for x in XY_corr_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mem_report(tensors, mem_type):\n",
    "        '''Print the selected tensors of type\n",
    "        There are two major storage types in our major concern:\n",
    "            - GPU: tensors transferred to CUDA devices\n",
    "            - CPU: tensors remaining on the system memory (usually unimportant)\n",
    "        Args:\n",
    "            - tensors: the tensors of specified type\n",
    "            - mem_type: 'CPU' or 'GPU' in current implementation '''\n",
    "        print('Storage on %s' %(mem_type))\n",
    "        print('-'*LEN)\n",
    "        total_numel = 0\n",
    "        total_mem = 0\n",
    "        visited_data = []\n",
    "        for tensor in tensors:\n",
    "            if tensor.is_sparse:\n",
    "                continue\n",
    "            # a data_ptr indicates a memory block allocated\n",
    "            data_ptr = tensor.storage().data_ptr()\n",
    "            if data_ptr in visited_data:\n",
    "                continue\n",
    "            visited_data.append(data_ptr)\n",
    "\n",
    "            numel = tensor.storage().size()\n",
    "            total_numel += numel\n",
    "            element_size = tensor.storage().element_size()\n",
    "            mem = numel*element_size /1024/1024 # 32bit=4Byte, MByte\n",
    "            total_mem += mem\n",
    "            element_type = type(tensor).__name__\n",
    "            size = tuple(tensor.size())\n",
    "\n",
    "            print('%s\\t\\t%s\\t\\t%.2f' % (\n",
    "                element_type,\n",
    "                size,\n",
    "                mem) )\n",
    "        print('-'*LEN)\n",
    "        print('Total Tensors: %d \\tUsed Memory Space: %.2f MBytes' % (total_numel, total_mem) )\n",
    "        print('-'*LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = gc.get_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN = 65\n",
    "print('%s\\t%s\\t\\t\\t%s' %('Element type', 'Size', 'Used MEM(MBytes)') )\n",
    "print('='*LEN)\n",
    "objects = gc.get_objects()\n",
    "print('%s\\t%s\\t\\t\\t%s' %('Element type', 'Size', 'Used MEM(MBytes)') )\n",
    "tensors = [obj for obj in objects if torch.is_tensor(obj)]\n",
    "cuda_tensors = [t for t in tensors if t.is_cuda]\n",
    "host_tensors = [t for t in tensors if not t.is_cuda]\n",
    "_mem_report(cuda_tensors, 'GPU')\n",
    "_mem_report(host_tensors, 'CPU')\n",
    "print('='*LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test cpu performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_name = 'gpt2-xl_ctrl_bert_gpt2_openaigpt_lm_1b_layers'\n",
    "optim_id='coordinate_ascent_eh-obj=D_s-n_iter=1000-n_samples=200-n_init=1-run_gpu=True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_id = ['group=gpt2-xl_layers-dataset=coca_spok_filter_punct_10K_sample_1-activation-bench=None-ave=False',\n",
    "                  'group=ctrl_layers-dataset=coca_spok_filter_punct_10K_sample_1-activation-bench=None-ave=False',\n",
    "                  'group=bert-large-uncased-whole-word-masking_layers-dataset=coca_spok_filter_punct_10K_sample_1-activation-bench=None-ave=False',\n",
    "                  'group=gpt2_layers-dataset=coca_spok_filter_punct_10K_sample_1-activation-bench=None-ave=False',\n",
    "                  'group=openaigpt_layers-dataset=coca_spok_filter_punct_10K_sample_1-activation-bench=None-ave=False',\n",
    "                  'group=lm_1b_layers-dataset=coca_spok_filter_punct_10K_sample_1-activation-bench=None-ave=False']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_obj = optim_pool[optim_id]()\n",
    "optim_group_obj = optim_group(n_init=optimizer_obj.n_init,\n",
    "                                  extract_group_name=extract_name,\n",
    "                                  ext_group_ids=extract_id,\n",
    "                                  n_iter=optimizer_obj.n_iter,\n",
    "                                  N_s=optimizer_obj.N_s,\n",
    "                                  objective_function=optimizer_obj.objective_function,\n",
    "                                  optim_algorithm=optimizer_obj.optim_algorithm,\n",
    "                                  run_gpu=optimizer_obj.run_gpu)\n",
    "# extract and constrcut low dim reprensetation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(SAVE_DIR,f\"{optim_group_obj.extract_group_name}_XY_corr_list.pkl\")):\n",
    "        D_precompute=load_obj(os.path.join(SAVE_DIR, f\"{optim_group_obj.extract_group_name}_XY_corr_list.pkl\"))\n",
    "        optim_group_obj.grp_XY_corr_list=D_precompute['grp_XY_corr_list']\n",
    "        optim_group_obj.N_S=D_precompute['N_S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f\"{x.device}\" for x in optim_group_obj.grp_XY_corr_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit optim_group_obj.gpu_obj_function(np.random.choice(optim_group_obj.N_S, size=optimizer_obj.N_s, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit cpu_obj_function(optim_group_obj.grp_XY_corr_list,np.random.choice(optim_group_obj.N_S, size=optimizer_obj.N_s, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_obj.N_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_corr_list=optim_group_obj.grp_XY_corr_list[0]\n",
    "samples = torch.tensor(np.random.choice(optim_group_obj.N_S, size=optimizer_obj.N_s, replace=False), dtype=torch.long)\n",
    "pairs = torch.combinations(samples, with_replacement=False)\n",
    "XY_corr_sample = [XY_corr[pairs[:, 0], pairs[:, 1]] for XY_corr in XY_corr_list]\n",
    "XY_corr_sample_tensor = torch.stack(XY_corr_sample).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XY_corr_sample_tensor = torch.stack(XY_corr_sample).float()\n",
    "  #  XY_corr_sample_tensor = torch.transpose(XY_corr_sample_tensor, 1, 0)\n",
    "  #  if XY_corr_sample_tensor.shape[1] < XY_corr_sample_tensor.shape[0]:\n",
    "    #    XY_corr_sample_tensor = torch.transpose(XY_corr_sample_tensor, 1, 0)\n",
    "assert (XY_corr_sample_tensor.shape[1] > XY_corr_sample_tensor.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit pt_create_corr_rdm_short(XY_corr_sample_tensor, device=XY_corr_sample_tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_corr_list=optim_group_obj.grp_XY_corr_list[0]\n",
    "XY_corr_list_1=torch.stack(XY_corr_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_corr_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S=np.random.choice(optim_group_obj.N_S, size=optimizer_obj.N_s, replace=False)\n",
    "samples = torch.tensor(S, dtype=torch.long)\n",
    "pairs = torch.combinations(samples, with_replacement=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b=[XY_corr[pairs[:, 0], pairs[:, 1]] for XY_corr in XY_corr_list]\n",
    "b = torch.stack(b).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=XY_corr_list_1[:,pairs[:, 0],pairs[:, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.all(torch.eq(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_corr_list_1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
